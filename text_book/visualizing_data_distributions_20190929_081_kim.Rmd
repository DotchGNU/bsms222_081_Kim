---
title: "Chap 9.Visualizing data distribution"
output: html_notebook
---

***

# **9.0 Intro**

Numerical data **--summarize-->** *average value*, *standard deviation*

Is this appropriate? Is there any important piece of information that we are *missing* by only looking at this summary rather than the entire list?

> It is better method to show entire distribution 
> There's a posibility of missing, but informative data. 

```
You may have noticed that numerical data is often summarized with the average value. For example, the quality of a high school is sometimes summarized with one number: the average score on a standardized test. Occasionally, a second number is reported: the standard deviation. For example, you might read a report stating that scores were 680 plus or minus 50 (the standard deviation). The report has summarized an entire vector of scores with with just two numbers. Is this appropriate? Is there any important piece of information that we are missing by only looking at this summary rather than the entire list?

Our first data visualization building block is learning to summarize lists of factors or numeric vectors. More often than not, the best way to share or explore this summary is through data visualization. The most basic statistical summary of a list of objects or numbers is its distribution. Once a vector has been summarized as a distribution, there are several data visualization techniques to effectively relay this information.

In this Chapter, we first discuss properties of a variety of distributions and how to visualize distributions using a motivating example of student heights. We then discuss the ggplot2 geometries for these visualizations in Section 9.16.
```



*** 

# **9.1 Variable types** 

Two types of variables 

1) categorical :  each entry in a vector comes from one of a small number of groups
 
    * ordinal : ordered categorical data 
    
    * non-ordinal

2) numeric 

    * discete : can be considered ordinal
    
    * continuous 

```
We will be working with two types of variables: categorical and numeric. Each can be divided into two other groups: categorical can be ordinal or not, whereas numerical variables can be discrete or continuous.

When each entry in a vector comes from one of a small number of groups, we refer to the data as categorical data. Two simple examples are sex (male or female) and regions (Northeast, South, North Central, West). Some categorical data can be ordered even if they are not numbers per se, such as spiciness (mild, medium, hot). In statistics textbooks, ordered categorical data are referred to as ordinal data.

Example of numerical data are population sizes, murder rates, and heights. Some numerical data can be treated as ordered categorical. We can further divide numerical data into continuous and discrete. Continuous variables are those that can take any value, such as heights, if measured with enough precision. For example, a pair of twins may be 68.12 and 68.11 inches respectively. Counts, such as population sizes, are discrete because they have to be round numbers.

Keep in mind that discrete numeric data can be considered ordinal. Although this is technically true, we usually reserve the term ordinal data for variables belonging to a small number of different groups, with each group having many members. In contrast, when we have many groups with few cases in each group, we typically refer to them as discrete numerical variables. So, for example, the number of packs of cigarettes a person smokes a day, rounded to the closest pack, would be considered ordinal, while the actual number of cigarettes would be considered a numerical variable. But, indeed, there are examples that can be considered both numerical and ordinal when it comes to visualizing data.
```



***

# **9.2 Case study : describing student heights**

```{r}
###describing student heights###

library(tidyverse)
library(dslabs)
data(heights)
head(heights)
```


```
Here we introduce a new motivating problem. It is an artificial one, but it will help us illustrate the concepts needed to understand distributions.

Pretend that we have to describe the heights of our classmates to ET, an extraterrestrial that has never seen humans. As a first step, we need to collect data. To do this, we ask students to report their heights in inches. We ask them to provide sex information because we know there are two different distributions by sex. We collect the data and save it in a data frame:

One way to convey the heights to ET is to simply send him this list of 1050 heights. But there are much more effective ways to convey this information, and understanding the concept of a distribution will help. To simplify the explanation, we first focus on male heights. We examine the female height data in Section 9.14 .
```


***

# **9.3 Distribution function ; for categorical variables**

> In some cases, the **average** and the **standard deviation** are pretty much all we need to understand the data.

> basic statistical summary of a list of object

* frequency table 

```{r}
heights %>% group_by(sex) %>%
    summarize(frequency = length(sex)/nrow(heights))
```


* simple barplot : if there are more categories, then a simple barplot describes the distribution. (for a few numbers of categories )

![simple barplot](https://rafalab.github.io/dsbook/book_files/figure-html/state-region-distribution-1.png)


```
It turns out that, in some cases, the average and the standard deviation are pretty much all we need to understand the data. We will learn data visualization techniques that will help us determine when this two number summary is appropriate. These same techniques will serve as an alternative for when two numbers are not enough.

The most basic statistical summary of a list of objects or numbers is its distribution. The simplest way to think of a distribution is as a compact description of a list with many entries. This concept should not be new for readers of this book. For example, with categorical data, the distribution simply describes the proportion of each unique category. The sex represented in the heights dataset is:

This two-category frequency table is the simplest form of a distribution. We don’t really need to visualize it since one number describes everything we need to know: 23% are females and the rest are males. When there are more categories, then a simple barplot describes the distribution. Here is an example with US state regions:

This particular plot simply shows us four numbers, one for each category. We usually use barplots to display a few numbers. Although this particular plot does not provide much more insight than a frequency table itself, it is a first example of how we convert a vector into a plot that succinctly summarizes all the information in the vector. When the data is numerical, the task of displaying distributions is more challenging.

```


***

# **9.4 Cumilative distribution functions ; for numerical variables**

> Cumulative distribution function(CDF) : reports the proportion of the data below $a$ for all possible values of $a$.

> $F(a) = Pr(x<=a)$

* a plot of $F$ for the male height data:

![cumulative function](https://rafalab.github.io/dsbook/book_files/figure-html/ecdf-1.png)

* $F(b)-F(a)$ : the proportion of values between any two heights, say a and b

> A final note: because CDFs can be defined mathematically the word empirical is added to make the distinction when data is used. We therefore use the term **empirical CDF (eCDF)**.

```
Numerical data that are not categorical also have distributions. In general, when data is not categorical, reporting the frequency of each entry is not an effective summary since most entries are unique. In our case study, while several students reported a height of 68 inches, only one student reported a height of 68.503937007874 inches and only one student reported a height 68.8976377952756 inches. We assume that they converted from 174 and 175 centimeters respectively.

Statistics textbooks teach us that a more useful way to define a distribution for numeric data is to define a function that reports the proportion of the data below a for all possible values of a. This function is called the cumulative distribution function (CDF). In statistics, the following notation is used:

F(a) = Pr(x<=a)

Here is a plot of  F  for the male height data:

Similar to what the frequency table does for categorical data, the CDF defines the distribution for numerical data. From the plot, we can see that 16% of the values are below 65, since F ( 66 ) = 0.164, or that 84% of the values are below 72, since F ( 72 ) = 0.841, etc.. In fact, we can report the proportion of values between any two heights, say a and b , by computing F ( b ) − F ( a ) . This means that if we send this plot above to ET, he will have all the information needed to reconstruct the entire list. Paraphrasing the expression “a picture is worth a thousand words”, in this case, a picture is as informative as 812 numbers. A final note: because CDFs can be defined mathematically the word empirical is added to make the distinction when data is used. We therefore use the term empirical CDF (eCDF).
```



***

# **9.5 Histograms**

> Histograms are much easier to interpret than CDF. 

* Histograms of non-overlappping bins

    + x axis : numerical (not categorical)
    
    + y axis : count as frequency

![histograms](https://rafalab.github.io/dsbook/book_files/figure-html/height-histogram-1.png)

> But it is less informative than CDF. 

* all values in each interval are treated the same when computing bin heights.

```
Although the CDF concept is widely discussed in statistics textbooks, the plot is actually not very popular in practice. The main reason is that it does not easily convey characteristics of interest such as: at what value is the distribution centered? Is the distribution symmetric? What ranges contain 95% of the values? Histograms are much preferred because they greatly facilitate answering such questions. Histograms sacrifice just a bit of information to produce plots that are much easier to interpret. The simplest way to make a histogram is to divide the span of our data into non-overlapping bins of the same size. Then, for each bin, we count the number of values that fall in that interval. The histogram plots these counts as bars with the base of the bar defined by the intervals. Here is the histogram for the height data splitting the range of values into one inch intervals: [ 49.5 , 50.5 ] , [ 51.5 , 52.5 ] , ( 53.5 , 54.5 ] , . . . , ( 82.5 , 83.5 ]

As you can see in the figure above, a histogram is similar to a barplot, but it differs in that the x-axis is numerical, not categorical.

If we send this plot to ET, he will immediately learn some important properties about our data. First, the range of the data is from 50 to 84 with the majority (more than 95%) between 63 and 75 inches. Second, the heights are close to symmetric around 69 inches. Also, by adding up counts, ET could obtain a very good approximation of the proportion of the data in any interval. Therefore, the histogram above is not only easy to interpret, but also provides almost all the information contained in the raw list of 812 heights with about 30 bin counts.

What information do we lose? Note that all values in each interval are treated the same when computing bin heights. So, for example, the histogram does not distinguish between 64, 64.1, and 64.2 inches. Given that these differences are almost unnoticeable to the eye, the practical implications are negligible and we were able to summarize the data to just 23 numbers.

We discuss how to code histograms in Section 9.16.
```



***

# **9.6 Smoothed density**

* Smoothed density

    + x axis : numerical (not categorical)
    
    + y axis : density as frequency (not count)

![Smoothed density](https://rafalab.github.io/dsbook/book_files/figure-html/example-of-smoothed-density-1.png)

> CDFs are empirical visualization, wherase Smooth density plots are estimated visualization.

* *estimates* 

    + assume that our list of observed values is a subset of a much larger list of unobserved values.
    
    + bins of size 1 :
    
    ![size 1](https://rafalab.github.io/dsbook/book_files/figure-html/simulated-data-histogram-1-1.png)
    
    + adjusted binwidth
    
    ![adjusted](https://rafalab.github.io/dsbook/book_files/figure-html/simulated-data-histogram-2-1.png)
    
    
    + very, very small bins 
    
    ![smooth](https://rafalab.github.io/dsbook/book_files/figure-html/simulated-density-1-1.png)
    
* *smoothness* of the curve can be controlled.

    ![smoothness](https://rafalab.github.io/dsbook/book_files/figure-html/densities-different-smoothness-1.png)
    
    + We should select a degree of smoothness that we can defend as being representative of the underlying data. 

```
Smooth density plots are aesthetically more appealing than histograms. Here is what a smooth density plot looks like for our heights data:

In this plot, we no longer have sharp edges at the interval boundaries and many of the local peaks have been removed. Also, the scale of the y-axis changed from counts to density.

To understand the smooth densities, we have to understand estimates, a topic we don’t cover until later. However, we provide a heuristic explanation to help you understand the basics so you can use this useful data visualization tool.

The main new concept you must understand is that we assume that our list of observed values is a subset of a much larger list of unobserved values. In the case of heights, you can imagine that our list of 812 male students comes from a hypothetical list containing all the heights of all the male students in all the world measured very precisely. Let’s say there are 1,000,000 of these measurements. This list of values has a distribution, like any list of values, and this larger distribution is really what we want to report to ET since it is much more general. Unfortunately, we don’t get to see it.

However, we make an assumption that helps us perhaps approximate it. If we had 1,000,000 values, measured very precisely, we could make a histogram with very, very small bins. The assumption is that if we show this, the height of consecutive bins will be similar. This is what we mean by smooth: we don’t have big jumps in the heights of consecutive bins. Below we have a hypothetical histogram with bins of size 1:

The smaller we make the bins, the smoother the histogram gets. Here are the histograms with bin width of 1, 0.5 and 0.1:

The smooth density is basically the curve that goes through the top of the histogram bars when the bins are very, very small. To make the curve not depend on the hypothetical size of the hypothetical list, we compute the curve on frequencies rather than counts:

Now, back to reality. We don’t have millions of measurements. Instead, we have 812 and we can’t make a histogram with very small bins.

We therefore make a histogram, using bin sizes appropriate for our data and computing frequencies rather than counts, and we draw a smooth curve that goes through the tops of the histogram bars. The following plots demonstrate the steps that lead to a smooth density:

However, remember that smooth is a relative term. We can actually control the smoothness of the curve that defines the smooth density through an option in the function that computes the smooth density curve. Here are two examples using different degrees of smoothness on the same histogram:

We need to make this choice with care as the resulting visualizations can change our interpretation of the data. We should select a degree of smoothness that we can defend as being representative of the underlying data. In the case of height, we really do have reason to believe that the proportion of people with similar heights should be the same. For example, the proportion that is 72 inches should be more similar to the proportion that is 71 than to the proportion that is 78 or 65. This implies that the curve should be pretty smooth; that is, the curve should look more like the example on the right than on the left.

While the histogram is an assumption-free summary, the smoothed density is based on some assumptions.
```


## 9.6.1 interpreting the y-axis

* Area : the proportion of data in the interval

* the area under the density curve adds up to 1

```
Note that interpreting the y-axis of a smooth density plot is not straightforward. It is scaled so that the area under the density curve adds up to 1. If you imagine we form a bin with a base 1 unit in length, the y-axis value tells us the proportion of values in that bin. However, this is only true for bins of size 1. For other size intervals, the best way to determine the proportion of data in that interval is by computing the proportion of the total area contained in that interval. For example, here are the proportion of values between 65 and 68:

The proportion of this area is about 0.3, meaning that about that proportion is between 65 and 68 inches.

By understanding this, we are ready to use the smooth density as a summary. For this dataset, we would feel quite comfortable with the smoothness assumption, and therefore with sharing this aesthetically pleasing figure with ET, which he could use to understand our male heights data:
```

## 9.6.2 Densities permit stratification 

![stratification](https://rafalab.github.io/dsbook/book_files/figure-html/two-densities-one-plot-1.png)

```
As a final note, we point out that an advantage of smooth densities over histograms for visualization purposes is that densities makes it easier to compare two distributions. This is in large part because the jagged edges of the histogram add clutter. Here is an example comparing male and female heights:

With the right argument, ggplot automatically shades the intersecting region with a different color. We will show examples of ggplot2 code for densities in Section 10 as well as Section 9.16.
```


***

# **9.7 Exercises**

1. In the `murders` dataset, the region is a categorical variable and the following is its distribution:

![distribution](https://rafalab.github.io/dsbook/book_files/figure-html/barplot-exercise-1.png)

To the closet 5%, what proportion of the states are in the North Central region?

> ###### ~0.2(?, frankly I'm not clearly understand this question...)



2. Which of the following is true:

A. The graph above is a histogram.

**B. The graph above shows only four numbers with a bar plot.**

C. Categories are not numbers, so it does not make sense to graph the distribution.

D. The colors, not the height of the bars, describe the distribution.



3. The plot below shows the eCDF for male heights:

![eCDF](https://rafalab.github.io/dsbook/book_files/figure-html/ecdf-exercise-1.png)

Based on the plot, what percentage of males are shorter than 75 inches?

A. 100%

**B. 95%**

C. 80%

D. 72 inches



4. To the closest inch, what height m has the property that 1/2 of the male students are taller than m and 1/2 are shorter?

A. 61 inches

B. 64 inches

**C. 69 inches**

D. 74 inches



5. Here is an eCDF of the murder rates across states:

![eCDF](https://rafalab.github.io/dsbook/book_files/figure-html/ecdf-exercise-2-1.png)

Knowing that there are 51 states (counting DC) and based on this plot, how many states have murder rates larger than 10 per 100,000 people?

**A. 1**

B. 5

C. 10

D. 50



6. Based on the eCDF above, which of the following statements are true:

A. About half the states have murder rates above 7 per 100,000 and the other half below.

B. Most states have murder rates below 2 per 100,000.

C. All the states have murder rates above 2 per 100,000.

**D. With the exception of 4 states, the murder rates are below 5 per 100,000.**



7. Below is a histogram of male heights in our heights dataset:

![histogram](https://rafalab.github.io/dsbook/book_files/figure-html/height-histogram-exercise-1.png)

Based on this plot, how many males are between 63.5 and 65.5?

A. 10

B. 24

**C. 34**

D. 100



8. About what percentage are shorter than 60 inches?

**A. 1%**

B. 10%

C. 25%

D. 50%



9. Based on the density plot below, about what proportion of US states have populations larger than 10 million?

![density](https://rafalab.github.io/dsbook/book_files/figure-html/density-exercise-1.png)

A. 0.02

**B. 0.15**

C. 0.50

D. 0.55

* y-axis scale 오류 아닌가? 



10. Below are three density plots. Is it possible that they are from the same dataset?

![plots](https://rafalab.github.io/dsbook/book_files/figure-html/density-exercise-2-1.png)

Which of the following statements is true:

**A. It is impossible that they are from the same dataset.**

B. They are from the same dataset, but the plots are different due to code errors.

C. They are the same dataset, but the first and second plot undersmooth and the third oversmooths.

D. They are the same dataset, but the first is not in the log scale, the second undersmooths and the third oversmooths.



***

# **9.8 The normal distribution**

> Normal distribution(Gaussian distribution) : two value summary_the average and standard deviation used as summary statistics

    * average(mean, $m$)

```
#definition of average
m <- sum(x) / length(x)
```
    
    * standard deviation(DS ,$s$)

```
#definition of standard deviation
s <- sqrt(sum((x-mu)^2) / length(x))
```

> Standard normal distribution : $m=0$, $s=1$

<center>![normal distribution](https://rafalab.github.io/dsbook/book_files/figure-html/normal-distribution-density-1.png)</center>

> Compare the normal distribution and empirical density plot 



```{r}
#boolean
index <- heights$sex == "Male"
x <- heights$height[index]

#calculate mean and sd by pre-built function
m <- mean(x)
s <- sd(x)

c(average = m, sd = s)
```

![comparison](https://rafalab.github.io/dsbook/book_files/figure-html/data-and-normal-densities-1.png)

```
Histograms and density plots provide excellent summaries of a distribution. But can we summarize even further? We often see the average and standard deviation used as summary statistics: a two-number summary! To understand what these summaries are and why they are so widely used, we need to understand the normal distribution. The normal distribution, also known as the bell curve and as the Gaussian distribution, is one of the most famous mathematical concepts in history. A reason for this is that approximately normal distributions occur in many situations, including gambling winnings, heights, weights, blood pressure, standardized test scores, and experimental measurement errors. There are explanations for this, but we describe these later. Here we focus on how the normal distribution helps us summarize data. Rather than using data, the normal distribution is defined with a mathematical formula. For any interval ( a , b ) , the proportion of values in that interval can be computed using this formula:

You don’t need to memorize or understand the details of the formula. But note that it is completely defined by just two parameters: m and s . The rest of the symbols in the formula represent the interval ends that we determine, a and b , and known mathematical constants π and e . These two parameters, m and s , are referred to as the average (also called the mean) and the standard deviation (SD) of the distribution respectively. The distribution is symmetric, centered at the average, and most values (about 95%) are within 2 SDs from the average. Here is what the normal distribution looks like when the average is 0 and the SD is 1:

The fact that the distribution is defined by just two parameters implies that if a dataset is approximated by a normal distribution, all the information needed to describe the distribution can be encoded in just two numbers: the average and the standard deviation. We now define these values for an arbitrary list of numbers.

For a list of numbers contained in a vector x, the average is defined as:

and the SD is defined as:

which can be interpreted as the average distance between values and their average. Let’s compute the values for the height for males which we will store in the object x :

Here is a plot of the smooth density and the normal distribution with mean = 69.3 and SD = 3.6 plotted as a black line with our student height smooth density in blue:

The normal distribution does appear to be quite a good approximation here. We now will see how well this approximation works at predicting the proportion of values within intervals.
```


***

# **9.9 Standard units**

> standard unit : `z = (x - m)/s`

    * value `x` from a vector `x`

```{r}
#convert to standard unit
z <- scale(x)
class(z)
```

```{r}
#calculate the proportion by standard deviation
#number '2' stands for standard deviation
mean(abs(z) < 2)
```

    * the proportion is predicted by the normal distribution
    
    * To further confirm that we can use quantile-quantile plots for approximation 

```
For data that is approximately normally distributed, it is convenient to think in terms of standard units. The standard unit of a value tells us how many standard deviations away from the average it is. Specifically, for a value x from a vector X, we define the value of x in standard units as z = (x - m)/s with m and s the average and standard deviation of X respectively. Why is this convenient? 

First look back at the formula for the normal distribution and note that what is being exponentiated is − z 2 / 2 with z equivalent to x in standard units. Because the maximum of e − z 2 / 2 is when z = 0 , this explains why the maximum of the distribution occurs at the average. It also explains the symmetry since − z 2 / 2 is symmetric around 0. Second, note that if we convert the normally distributed data to standard units, we can quickly know if, for example, a person is about average ( z = 0 ), one of the largest ( z ≈ 2 ), one of the smallest ( z ≈ − 2 ) or an extremely rare occurrence ( z > 3 or z < − 3 ). Remember that it does not matter what the original units are, these rules apply to any data that is approximately normal. 

In R, we can obtain standard units using the function scale:

Now to see how many men are within 2 SDs from the average, we simply type:

The proportion is about 95%, which is what the normal distribution predicts! To further confirm that, in fact, the approximation is a good one, we can use quantile-quantile plots.
```


***

# **9.10 Quantile-quantile plots(QQ plots)**

> QQ plots : A systematic way to assess how well the normal distribution fits the data is to check if the **observed** and **predicted** proportions match.

* $\Phi(x)$ : the probability of a standard normal distribution being smaller than `x`

    + e.g. $\Phi(-1.96)=0.025$, $\Phi(1.96)=0.975$  

    + In R, we can evaluate $\Phi$ using the `pnorm` function 
    
```{r}
pnorm(-1.96)
```

* Theoretical quantile for the normal distribution : $\Phi^{-1}(x)$, calculation related to quantiles done without data

    + e.g. $\Phi^{-1}(0.975)=1.96$ 
    
    + In R, we can evaluate the inverse of $\Phi$ using the `qnorm` function
    
```{r}
# return statistical varibles by proportion for the standard normal distribution by default 
qnorm(0.975)
```

* for any normal distribution : using the `mean` and `sd` arguments in the `pnorm` and `qnorm`

```{r}
qnorm(0.975, mean = 5, sd = 2)
```


* Theoretical quantile for any data in a vector $x$ : any proportion $p$ as the $q$ for which the proportion of values below $q$

    + In R, `q` can be defined as `mean(x <= q) = p`
    
    + It's also evalusted by `quantile` function
    
```{r}
# it's boolean
x <= 69.5

# it's the proportion below the variables 
mean(x <= 69.5)
```

* **QQ-plot** : comparing the observed quantile and expected quantile(by normal distribution)

```{r}
# seq(start, end, interval)
p <- seq(0.05, 0.95, 0.05)
```

```{r}
# calculate empirical quantile(observed)
# quantile(data vector, qunatile vector)
sample_quantiles <- quantile(x, p)

sample_quantiles
```

```{r}
# calculate theoretical quantile(exprected)
theoretical_quantiles <- qnorm(p, mean = mean(x), sd = sd(x))
theoretical_quantiles
```

```{r}
qplot(theoretical_quantiles, sample_quantiles) + geom_abline()
```

    + using **ggplot2**
    
```{r}
# the default of the number of quantiles is maximum 
heights %>% filter(sex=="Male") %>%
    ggplot(aes(sample = scale(height))) +
    geom_qq() +
    geom_abline()
```



***

# **9.11 Percentiles**

* percentiles : special cases of quantiles. $q$(statistical variables) from any $p$ 

    + quantiles are obtained when setting $p = 0.25, 0.50, 0.75$ 
    
```
Before we move on, let’s define some terms that are commonly used in exploratory data analysis.
Percentiles are special cases of quantiles that are commonly used. The percentiles are the quantiles you obtain when setting the p at 0.01 , 0.02 , . . . , 0.99 . We call, for example, the case of p = 0.25 the 25th percentile, which gives us a number for which 25% of the data is below. The most famous percentile is the 50th, also known as the median. 
For the normal distribution the median and average are the same, but this is generally not the case. 
Another special case that receives a name are the quartiles, which are obtained when setting p = 0.25 , 0.50 , and 0.75 .
```


***

# **9.12 Boxplots**

> Boxplots : 5 number summary of dataset with the quantiles and the outliers(out of interquantile range)

*** 

# **9.13 Stratification**

* Stratification : divide observations into groups based on the values of one or more variables associated with those observations(clustering)

    + strata : the resulting group


```
In data analysis we often divide observations into groups based on the values of one or more variables associated with those observations. For example in the next section we divide the height values into groups based on a sex variable: females and males. We call this procedure stratification and refer to the resulting groups as strata.

Stratification is common in data visualization because we are often interested in how the distribution of variables differ across different subgroups. We will see several examples throughout this part of the book. We will revisit the concept of stratification when we learn regression in Chapter 18 and in the Machine Learning part of the book.
```


***

# **9.14 Case study : describing student heights (continued)**


```{r}
heights %>% group_by(sex) %>%
    ggplot(aes(x = sex, y = height)) +
    geom_boxplot()
```


```{r}
## the upper out-lier of female
heights %>% filter(sex=="Female") %>%
    top_n(5, desc(height)) %>%
    pull(height)
```



***

# **9.15 Exercises**

1. Define variables containing the heights of males and females like this:

```{r}
library(dslabs)
data(heights)
male <- heights$height[heights$sex=="Male"]
female <- heights$height[heights$sex=="Female"]
```

How many measurements do we have for each?

```{r}
c(male = length(male), female = length(female))
```



2. Suppose we can’t make a plot and want to compare the distributions side by side. We can’t just list all the numbers. Instead, we will look at the percentiles. Create a five row table showing `female_percentiles` and `male_percentiles` with the 10th, 30th, 50th, …, 90th percentiles for each sex. Then create a data frame with these two as columns.

```{r}
p <- seq(0.1,0.9, 0.1)
data_frame(percentile = p, male_percentile = quantile(male, p), female_percentile = quantile(female, p))
```



3. Study the following boxplots showing population sizes by country:

![image](https://rafalab.github.io/dsbook/book_files/figure-html/boxplot-exercise-1.png)

Which continent has the country with the biggest population size?

> ###### Asia



4. What continent has the largest median population size?

> ###### Africa



5. What is median population size for Africa to the nearest million?

> ###### ~10



6. What proportion of countries in Europe have populations below 14 million?

A. 0.99

**B. 0.75**

C. 0.50

D. 0.25



7. If we use a log transformation, which continent shown above has the largest interquartile range?

> ####### America



8. Load the height data set and create a vector `x` with just the male heights:

```{r}
x <- heights$height[heights$sex=="Male"]
```

What proportion of the data is between 69 and 72 inches (taller than 69, but shorter or equal to 72)? Hint: use a logical operator and `mean`.

```{r}
q <- 69 < x &  x<= 72
mean(q)
```



9. Suppose all you know about the data is the average and the standard deviation. Use the normal approximation to estimate the proportion you just calculated. 
Hint: start by computing the average and standard deviation. Then use the `pnorm` function to predict the proportions.

```{r}
m <- mean(x)
s <- sd(x)
p <- pnorm(c(72,69), mean = m, sd = s)
p[1] - p[2]
```



10. Notice that the approximation calculated in question two is very close to the exact calculation in the first question. Now perform the same task for more extreme values. Compare the exact calculation and the normal approximation for the interval (79,81]. How many times bigger is the actual proportion than the approximation?

```{r}
q <- 79 < x & x <= 81
p_ob <- mean(q)
p <- pnorm(c(81,79), mean = mean(x), sd = sd(x))
p_ex <- p[1] - p[2]
p_ob / p_ex
```


11. Approximate the distribution of adult men in the world as normally distributed with an average of 69 inches and a standard deviation of 3 inches. Using this approximation, estimate the proportion of adult men that are 7 feet tall or taller, referred to as seven footers. Hint: use the `pnorm` function.


```{r}
# 1feet = 12inch
p_7feet <- 1- pnorm(84, mean = 69, sd = 3)
p_7feet
```


12. There are about 1 billion men between the ages of 18 and 40 in the world. Use your answer to the previous question to estimate how many of these men (18-40 year olds) are seven feet tall or taller in the world?

```{r}
as.integer(1 * 10^9 * p_7feet) 
```

13. There are about 10 National Basketball Association (NBA) players that are 7 feet tall or higher. Using the answer to the previous two questions, what proportion of the world’s 18 to 40 year old seven footers are in the NBA?

```{r}
10 / (1*10^9*p_7feet)
```


14. Repeat the calculations performed in the previous question for Lebron James’ height: 6 feet 8 inches. There are about 150 players that are at least that tall.

```{r}
# 6feet 8inches = 80inches
p_J <- pnorm(80, mean = 69, sd = 3)
150 /(10^9*(1 - p_J))
```


15. In answering the previous questions, we found that it is not at all rare for a seven footer to become an NBA player. What would be a fair critique of our calculations:

A. Practice and talent are what make a great basketball player, not height.

B. The normal approximation is not appropriate for heights.

**C. As seen in question 3, the normal approximation tends to underestimate the extreme values. It’s possible that there are more seven footers than we predicted.**

D. As seen in question 3, the normal approximation tends to overestimate the extreme values. It’s possible that there are less seven footers than we predicted.



***

# **9.16 ggplot2 geometries**

## 9.16.1 Barplots

```{r}
murders %>% ggplot(aes(region)) + geom_bar()
```


```{r}
murders %>% count(region) %>% 
    mutate(proportion = n/sum(n)) %>%
    ggplot(aes(region, proportion)) +
    geom_bar(stat = "identity")
# stat = "identity" : provide x and y to geom_bar
```



## 9.16.2 Histograms

```{r}
heights %>%
    filter(sex == "Female") %>%
    ggplot(aes(height)) +
    geom_histogram()
```

* adjust binwidth with `binwidth`

```{r}
```{r}
heights %>%
    filter(sex == "Female") %>%
    ggplot(aes(height)) +
    geom_histogram(binwidth = 1)
```


```{r}
heights %>%
    filter(sex == "Female") %>%
    ggplot(aes(height)) +
    geom_histogram(binwidth = 1, fill = "blue", col = "black") +
    xlab("Female heights in inches") + 
    ggtitle("Histogram")
```



## 9.16.3 Density plots

```{r}
heights %>%
    filter(sex == "Female") %>%
    ggplot(aes(height)) +
    geom_density(fill = "blue")
```

* adjust smoothness by `adjust = ` argument.

```{r}
heights %>%
    filter(sex == "Female") %>%
    ggplot(aes(height)) +
    geom_density(fill = "blue", adjust = 2)
```




## 9.16.4 Boxplots

```{r}
heights %>% group_by(sex) %>%
    ggplot(aes(sex, height)) +
    geom_boxplot()
```



## 9.16.5 QQ-plots 

* `geom_qq()`

    + By default, the sample variable is compared to a normal distribution with average 0 and standard deviation 1.
    
    + `dparams` : adjust the type of normal distribution 
    
    + `geom_abline` : trend lin (default : slope=1, intercept=0)
    
    + `scale()` method of `aes` : scale the data first and then make a qqplot against the standard normal.

```{r}
heights %>% filter(sex=="Female") %>%
    ggplot(aes(sample = height)) +
    geom_qq()
```


```{r}
# calculate parameter for normal distribution
params <- heights %>% filter(sex=="Female") %>%
    summarize(mean = mean(height), sd = sd(height))

heights %>% filter(sex == "Female") %>%
    ggplot(aes(sample = height)) +
    geom_qq(dparams = params) +
    geom_abline()
```



## 9.16.6 Images 

> **geom_tile**
> **geom_raster** 

```{r}
x <- expand.grid(x = 1:12, y = 1:10) %>%
    mutate(z = 1:120)
```

```{r}
matrix(1:120, 12, 10)
```

```{r}
x %>% ggplot(aes(x, y, fill = z)) +
    geom_raster()
```

```{r}
x %>% ggplot(aes(x, y, fill = z)) +
    geom_raster() +
    scale_fill_gradientn(colors = terrain.colors(10))
```



## 9.16.7 Quick plots 

> Quick plots은 적폐입니다. 

```{r}
x <- heights %>%
    filter(sex == "Male") %>%
    pull(height)
qplot(x)
```

```{r}
qplot(sample = scale(x)) + geom_abline()
```

```{r}
heights %>% qplot(sex, height, data = .)
```



```{r}
heights %>% qplot(sex, height, data = ., geom = "boxplot")
```

```{r}
qplot(x, geom = "density")
```



```{r}
qplot(x, bins = 15, color = I("black"), xlab = "Population")
```

* Technical note : `I("black")`, `aes`에 의하여 factor로 자동변환되는 것을 방지 

> function `I` in R : "keep it as it is" 



***

# 9.17 Exercises 

1. Now we are going to use the `geom_histogram` function to make a histogram of the `heights` in the height data frame. When reading the documentation for this function we see that it requires just one mapping, the values to be used for the histogram. Make a histogram of all the plots.
What is the variable containing the heights?

A. sex

B. heights

**C. height**

D. heights$height



2. Now create a ggplot object using the pipe to assign the heights data to a ggplot object. Assign `height` to the x values through the `aes` function.

```
heights %>% ggplot(aes(height)) 
```


3. Now we are ready to add a layer to actually make the histogram. Use the object created in the previous exercise and the `geom_histogram` function to make the histogram.

```{r}
heights %>% ggplot(aes(height)) +
    geom_histogram()
```



4. Note that when we run the code in the previous exercise we get the warning: `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.

Use the `binwidth` argument to change the histogram made in the previous exercise to use bins of size 1 inch.

```{r}
heights %>% ggplot(aes(height)) +
    geom_histogram(binwidth = 1)
```


5. Instead of a histogram, we are going to make a smooth density plot. In this case we will not make an object, but instead render the plot with one line of code. Change the geometry in the code previously used to make a smooth density instead of a histogram.

```{r}
heights %>% ggplot(aes(height)) + geom_density()
```


6. Now we are going to make a density plot for males and females separately. We can do this using the `group` argument. We assign groups via the aesthetic mapping as each point needs to a group before making the calculations needed to estimate a density.

```{r}
heights %>% ggplot(aes(height,group=sex)) + geom_density()
```



7. We can also assign groups through the `color` argument. This has the added benefit that it uses color to distinguish the groups. Change the code above to use color.

```{r}
heights %>% ggplot(aes(height,group=sex, color=sex)) + geom_density()
```



8. We can also assign groups through the fill argument. This has the added benefit that it uses colors to distinguish the groups, like this:

```{r}
heights %>% 
    ggplot(aes(height, fill=sex)) +
    geom_density()
```

However, here the second density is drawn over the other. We can make the curves more visible by using alpha blending to add transparency. Set the alpha parameter to 0.2 in the geom_density function to make this change.

```{r}
heights %>% 
    ggplot(aes(height, color=sex, fill=sex)) +
    geom_density(alpha = 0.2)
```

