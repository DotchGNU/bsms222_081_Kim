---
title: "Chap 13 Probability"
output: html_notebook
---

**Probability theory** is useful in many other contexts and, in particular, in areas that depend **on data affected by chance** in some way. All of the other chapters in this part build upon probability theory. Knowledge of probability is therefore indispensable for data science.

***

# **13.1. Discrete probability**

- Categorical data => Discrete probability 

- Numeric data, Continuous data 

## 13.1.1 Relative frequency 

- Definition : the proportion of times the event occurs when we repeat the experiment an infinite number of times, independently, and under the same conditions. 

## 13.1.2 Notation 

- `Pr(A)` : the probability of event `A` happening

=> can write events in a more mathematical form

## 13.1.3 Probabiltiy distribution

- relative frequency of the differenct categories => assign a probability to each category 

# **13.2. Monte Carlo simulations for categorical data**

 Random number generators permit us to mimic the process of picking at random.
 
```{r}
beads <- rep(c("red", "blue"), times = c(2,3)) #generate the urn
beads
```

```{r}
sample(beads, 1) #random picking
```

> repeat the experiment a large enough number of times to make the results practically equivalent to repeating forever : **Monte Carlo simulation**

- what is 'large enough' --> discuss later 

* `replicate`

```{r}
B <- 10000
events <- replicate (B, sample(beads, 1)) # repeat the same task
```

* `table`

```{r}
tab <- table(events)
tab
```

* `prop.table`

```{r}
prop.table(tab)
```

The numbers above are the estimated probabilities provided by this Monte Carlo simulation.

We use simple ones to demonstrate the computing tools available in R.

## 13.2.1 Setting the random seed

* Random Number Generator (RNG) 

```{r}
set.seed(1986)
```

```{r}
?set.seed
```

## 13.2.2 With and without replacement 

* `sample` : occurs without replacement 

 - `sample`함수의 default는 picking 후 다시 urn을 채우지 않는 것 

```{r}
sample(beads,5)
```

```{r}
sample(beads,5)
```

```{r}
sample(beads,5)
```

```{r}
sample(beads,6)
```


* picking with replacement

 - it can be used `replicate`

```{r}
events <- sample(beads, B, replace = TRUE)
prop.table(table(events))
```

# **13.3 Independence**

- Def : if the outcome of one does not affect the other 

e.g. coin tosses 

- 'not independent' : the first outcome affected the next one 

To see an extreme case of non-independent events, consider our example of drawing five beads at random without replacement:

```{r}
x <- sample(beads, 5)
```

If you have to guess the color of the first bead, you will predict blue since blue has a 60% chance. But if I show you the result of the last four outcomes:

```{r}
x[2:5]
````

would you still guess blue? Of course not. Now you know that the probability of red is 1 since the only bead left is red. The events are not independent, so the probabilities change.

# **13.4 Conditional probabilities**

When events are not independent, **conditional probabilities** are useful.

- We use the  ∣  as shorthand for “given that” or “conditional on”.

When two events, say $A$ and $B$, are independent, we habe:

$Pr(A|B) = Pr(A)$

This is the mathematical way of saying: the fact that `B` happened does not affect the probability of `A`  happening. In fact, this can be considered the mathematical definition of independence.

# **13.5 Addition and multiplication rules**

## 13.5.1 Multiplication rule

$Pr(A and B) = Pr(A)Pr(B|A)$

==> $Pr(A and B and C) = Pr(A)Pr(B|A)Pr(C|A and B)$

## 13.5.2 Multiplication rule under independence 

세 사건 A, B, C가 독립일 때,

$Pr(A and B and C) = Pr(A)Pr(B)Pr(C)$

* 조건부확률에 대한 일반적인 공식

$Pr(B|A) = Pr(A and B) / Pr(A)$

## 13.5.3 Addition rule

$Pr(A or B) = Pr(A) + Pr(B) - Pr(A and B)$

# **13.6 Combinations and permutation**

let's construct a deck of cards by `paste` and `expand.grid`

```{r}
numbers <- "Three"
suit <- "Hearts"
paste(numbers, suit)
```

`paste also works on pairs of vectors performing the operation element-wise: 

```{r}
paste(letters[1:5], as.character(1:5))
```

`expand.grid`를 통한 두 벡터의 모든 조합 확장 

```{r}
expand.grid(pants= c("blue", "black"), shirt = c("white", "grey", "plaid"))
```

```{r}
suits <- c("Diamonds", "Clubs", "Hearts", "Spades")
numbers <- c("Ace", "Deuce", "Three", "Four", "Five", "Six", "Seven", "Eight", "Nine", " Ten", "Jack", "Queen", "King")
deck <- expand.grid(number=numbers, suit=suits)
deck <- paste(deck$number, deck$suit)
```

```{r}
deck
```

```{r}
kings <- paste("King", suits)
mean(deck %in% kings)
```

* 조건부확률 연산 : **gtools*의 `permutations`

 - For any list of size `n`, this function computes all the different combinations we can get when we select `r` items. 
 
```{r}
install.packages("gtools")
library(gtools)
permutations(3,2)
```

Notice that the order matters here: 3,1 is different than 1,3. Also, note that (1,1), (2,2), and (3,3) do not appear because once we pick a number, it can’t appear again.

Optionally, we can add a vector. If you want to see five random seven digit phone numbers out of all possible phone numbers (without repeats), you can type:

```{r}
all_phone_numbers <- permutations(10, 7, v=0:9)
n <- nrow(all_phone_numbers)
index <- sample(n, 5)
all_phone_numbers[index,]
```


- deck의 52개 원소 중 순서에 상관 있게 2개를 뽑는 모든 조합 

```{r}
hands <- permutations(52, 2, v=deck)
```

- 첫번째 뽑은 카드가 king인 경우 

```{r}
first_card <- hands[,1]
second_card <- hands[,2]
```

```{r}
kings <- paste("King", suits)
sum(first_card %in% kings)
```

- 두번째도 king을 뽑을 비율 계산

```{r}
sum(first_card %in% kings & second_card %in% kings) / sum(first_card %in% kings)
```

```{r}
mean(first_card %in% kings & second_card %in% kings) / mean(first_card %in% kings)
```

> $Pr(A|B) = Pr(A and B) / Pr(A)$

* 순서가 중요하지 않을 경우 : `combinations`

- 순서와 상관 없이 3 중 2개를 뽑을 경우
  
```{r}
combinations(3,2)
```

- Blackjack의 Natural 21의 확률을 계산하는 예시

```{r}
aces <- paste("Ace", suits)

facecard <- c("King", "Queen", "Jack", "Ten")
facecard <- expand.grid(number = facecard, suit = suits)
facecard <- paste(facecard$number, facecard$suit)

hands <- combinations(52, 2, v=deck)
mean(hands[,1] %in% aces & hands[,2] %in% facecard)
```

<=>

```{r}
mean((hands[,1] %in% aces & hands[,2] %in% facecard) | 
       (hands[,2] %in% aces & hands[,1] %in% facecard))
```

## 13.6.1 Monte Carlo example

- Monte Carlo simulation을 이용하여 위의 예시를 구현할 수 있음. 

```{r}
hand <- sample(deck, 2) #deck에서 반복을 허용하지 않고 random 2개 뽑기 
hand
```

```{r}
(hands[1] %in% aces & hands[2] %in% facecard |
   hands[2] %in% aces & hands[1] %in% facecard)
```

- 위와 같은 sampling을 10,000ghl qksqhr -> Natural 21의 확률에 다가갈 것. 

```{r}
blackjack <- function(){
  hand <- sample(deck, 2)
  (hand[1] %in% aces & hand[2] %in% facecard |
      hand[2] %in% aces & hand[1] %in% facecard)
}
```

```{r}
blackjack()
```

- Montecarlo simulation을 통하여 추정한 Natural 21이 나올 확률 

```{r}
B <- 10000
results <- replicate(B, blackjack())
mean(results)
```


# **13.7. Examples**

## 13.7.1 Monty Hall problem

```{r}
B <- 10000

monty_hall <- function(strategy){
  doors <- as.character(1:3)
  prize <- sample(c("car", "goat", "goat")) 
  prize_door <- doors[prize == "car"] # index of prize_door
  
  my_pick <- sample(doors, 1)
  show <- sample(doors[!doors %in% c(my_pick, prize_door)],1)
  
  stick <- my_pick
  stick == prize_door
  switch <- doors[!doors %in% c(my_pick, show)]
  
  choice <- ifelse(strategy == "stick", stick, switch)
  choice == prize_door
}
```

```{r}
stick <- replicate(B, monty_hall("stick")) #선택을 바꾸지 않을 때m
mean(stick)
```

```{r}
switch <- replicate(B, monty_hall("switch"))
mean(switch)
```

## 13.7.2 Birthday problem 

```{r}
n <- 50
bdays <- sample(1:365, n, replace = TRUE)
```

```{r}
duplicated(c(1,2,3,1,4,3,5)) #vector에서 이전에 자기 자신과 동일한 element가 있는지 확인
```

```{r}
any(duplicated(bdays))
```

```{r}
B <- 10000

same_birthday <- function(n){
  bdays <- sample(1:365, n, replace=TRUE)
  any(duplicated(bdays))
}
results <- replicate(B, same_birthday(50))
mean(results)
```

```{r}
compute_prob <- function(n, B=10000){
  results <- replicate(B, same_birthday(n))
  mean(results)
}
```

```{r}
n <- seq(1,60)
prob <- sapply(n,compute_prob)
prob
```

```{r}
qplot(n, prob)
```

```{r}
exact_prob <- function(n){
  prob_unique <- seq(365, 365-n+1)/365
  1 - prod(prob_unique)
}
eprob <- sapply(n, exact_prob)
qplot(n, prob) + geom_line(aes(n, eprob), col="red")
```



# **13.8 Infinity in prctice**

```{r}
B <- 10^seq(1, 5, len=100)
compute_prob <- function(B, n=25){
  same_day <- replicate(B, same_birthday(n))
  mean(same_day)
}
prob <- sapply(B, compute_prob)
qplot(log10(B), prob, geom="line")
```

## **13.9 Exercises**

1. One ball will be drawn at random from a box containing : 3 cyan balls, 5 magenta balls, and 7 yellow balls. What is the probability that the ball will be cyan?

```{r}
balls <- rep(c("cyan", "magenta", "yellow"), times = c(3,5,7))
prop.table(table(sample(balls, 10000, replace = TRUE)))
```

 - (Frequentist probabilty) = 3/15 = 0.2  
 
 - (Bayesian probability) = 0.2012 (For 10000 trial)
 
2. What is the probability that the ball will not be cyan?
 
  - (Frequentist probability) = 0.8 
  
3. Instead of taking just one draw, consider taking two draws. You take the second draw without returning the first draw to the box. We call this sampling **without** replacement. What is the probability that the first draw is cyan and that the second draw is not cyan? 

```{r}
ballpick <- function(replace){
  hand <- sample(balls, 2, replace=replace)
  hand[1] == "cyan" & hand[2] != "cyan"
}
mean(replicate(10000, ballpick(replace=FALSE)))
```

  - (Frequentist probability) = 3/15 * 12/14 = 0.1714
  
  - (Bayesian probability) = 0.1653


4.  Now repeat the experiment, but this time, after taking the first draw and recording the color, return it to the box and shake the box. We call this sampling **with** replacement. What is the probability that the first draw is cyan and that the second draw is not cyan?

```{r}
mean(replicate(10000, ballpick(replace=TRUE)))
```

  - (Frequentist probability) = 3/15 * 12/15 = 0.16
  
  - (Baysian probability) = 0.1653
  
5. Two events $A$ and $B$ are independent if $Pr(A and B) = Pr(A)Pr(B)$. Under which situation are the draws independent?

  A. You don’t replace the draw.
  **B. You replace the draw.**
  C. Neither
  D. Both

6. Say you've drawn 5 balls from the box, with replacement, and all haven been yellow. What is the probability that the next one is yellow? 

  * 7/15
  
7. If you roll a 6-sided die six time, what is the probability of not seeing a 6? 

  * (5/6)^6 = 0.3349
  
8. Two teams, say the Ceitics and the Cavs, are playing a seven game series. The Cavs are a better team and have a 60% chance of winning each game. What is the probability that the Caeltics win at least one game? 

  * 1- (0.6)^7 = 0.9720
  
9. Create a Monte Carlo simulation to confirm your answer to the previous problem. Use `B <- 10000` simulations. 

```{r}
celltic_wins <- function(){
  celtic_wins <- sample(c(0,1), 7, replace = TRUE, prob= c(0.6, 0.4))
  ifelse(mean(celtic_wins) == 0, 0, 1) #celtic이 다지면 0 return, 1번이라도 이기면 1
}
mean(replicate(10000, celltic_wins()))
```

10. Two teams, say the Cavs and the Warriors, are playing a seven game championship series. The first to win four games, therefore, wins the series. The teams are equally good so they each have a 50-50 chance of winning each game. If the Cavs lose the first game, what is the probability that they win the series? 

 * 1 - (6C2 + 6C1 + 6C0) * (1/2)^6 = 0.65625
        남은 6경기 중 2경기만 이김. 1경기만 이김. 다 짐 

11. Confirm the results of the previous question with a Monte Carlo simulation

```{r}
cav_wins <- function(){
  cav_wins <- sample(c(0,1), 6, replace = TRUE, prob= c(0.5, 0.5))
  ifelse(mean(cav_wins) >= 0.5, 1, 0) #celtic이 다지면 0 return, 1번이라도 이기면 1
}
mean(replicate(10000, cav_wins()))
```

12. Two teams, $A$ and $B$, are playing a seven game series. Team $A$ is better than team $B$ and has a $p>0.5$ chance of winning each game. Givven a value $p$, the probability of winning the series for the underdog team $B$ can be computed with the following function based on a Monte Carlo simulation : 

```{r}
prob_win <- function(p){
  B <- 10000
  result <- replicate(B, {
    b_win <- sample(c(1,0), 7, replace = TRUE, prob = c(1-p,p))
    sum(b_win)>=4
  })
  mean(result)
}
```

Use the function `sapply` to compute the probability, call it `Pr`, of winning for `p<-seq(0.5, 0.95, 0.025)`. Then plot the result. 

```{r}
p <- seq(0.5, 0.95, 0.025)
Pr <- sapply(p, prob_win)
qplot(p, Pr)
```

13. Repeat the exercise above, but now keep the probability fixed at `p <- 0.75` and compute the probablity for different series lengths : best of 1game, 3games, 5games, ... Specifically, `N<-seq(1, 25, 2)`. 

```{r}
prob_win <- function(N, p=0.75){
  B <- 10000
  results <- replicate(B, {
    b_win <- sample(c(1,0), N, replace = TRUE, prob = c(1-p, p))
    sum(b_win) >= (N+1)/2
  })
  mean(result)
}
```

```{r}
N <- seq(1,25,2)
Pr2 <- sapply(N, prob_win)
qplot(N, Pr2)
```

- 팀의 승률이 일정하다면, 총 게임 수와 상관 없이 시리즈 승률은 일정 

***

# **13.10 Continuous probability**

* Continuous probability -(summary)-> using Cumulative Distribution Function(CDF)

```{r}
library(tidyverse)
library(dslabs)
data(heights)
x <- heights %>% filter(sex=="Male") %>% pull(height)
```

* defined the empirical distribution function as :

```{r}
F <- function(a) mean(x<=a)
```

* 남학생의 키가 70.5인치보다 큰 학생의 비율

```{r}
1 - F(70.5)
```

* 남학생의 키가 a와 b 사이에 있을 확률

```{markdown}
F(b)-F(a)
```


***
# **13.11 Theoretical continuous distributions**

* normal distribution -> approxcimation to many naturally occurring distributions

```{markdown}
F(a) = pnorm(a, m , s) #m=average, s=standard deviation
```

* 무작위로 선택된 학생의 키가 70.5인치보다 클 확률

```{r}
m <- mean(x)
s <- sd(x)
1 - pnorm(70.5, m, s)
```

## 13.11.1 Theoretical distributions as approximations

* 연속확률분포에서는 확률변수가 특정값을 가질 확률은 크게 의미가 없음. 

* 그보다 확률변수가 특정 구간에서 값을 가질 확률이 의미 있음. 

* 이를 이론적인 normal distribution으로 근사화하는 것이 의미 있음.

> empirical distribution 

```{r}
mean(x <= 68.5) - mean(x <= 67.5)
```

```{r}
mean(x <= 69.5) - mean(x <= 68.5)
```

```{r}
mean(x <= 70.5) - mean(x <= 69.5)
```

> normal approximation

```{r}
pnorm(68.5, m, s) - pnorm(67.5, m, s) 
```

```{r}
pnorm(69.5, m, s) - pnorm(68.5, m, s) 
```

```{r}
pnorm(70.5, m, s) - pnorm(69.5, m, s) 
```

> 해당 데이터는 반올림을 통하여 얻은 분포이므로 아래와 같은 구간에서는 근사값과 다른 값을 가짐

```{r}
mean(x <= 70.9) - mean(x <= 70.1)
```

```{r}
pnorm(70.9, m, s) - pnorm(70.1, m, s)
```

* In general, we call this situation **discretization**. . Although the true height distribution is continuous, the reported heights tend to be more common at discrete values, in this case, due to rounding. As long as we are aware of how to deal with this reality, the normal approximation can still be a very useful tool.

## 13.11.2 The probability density

* For categorical distributions, we can define the probability of a category.

$ F(a) = Pr(X<=a) = \int_{-\infty}^a f(x)ds $

* 정규분포로 근사되는 예시 

```{r}
1 - pnorm(76, m, s)
```

```{r}
data <- dnorm(x, mean=m, sd=s) 
ggplot(NULL, aes(c(60,80))) +
  stat_function(fun=dnorm, args=list(mean=m, sd=s),colour="blue", size=1) +
  geom_area(stat='function', fun=dnorm, args=list(mean=m, sd=s), xlim=c(qnorm(0.968, m, s), 80), fill='gray75') +
  ggtitle("Normal Distribution")
```

***
# **13.12 Monte Carlo simulations for continuous variables**

* `rnorm(size, average, sd)` : takes three arguments and produces random numbers 

```{r}
n <- length(x)
m <- mean(x)
s <- sd(x)
simulated_heights <- rnorm(n, m, s)
simulated_heights %>% qplot()
```

It will permit us to generate data that mimics natural events and answers questions related to what could happen by chance by **running Monte Carlo simulations**.

```{r}
B <- 10000

tallest <- replicate(B, {
  simulated_data <- rnorm(800, m, s)
  max(simulated_data)
})
```

```{r}
mean(tallest >= 7*12)
```

```{r}
tallest %>% qplot()
```

***
# **13.13 Continuous distributions**

* Normal distribution 외에도 student-t, Chi-square, exponential, gamma, beta, and beta-binomial distribution과 같이 다른 연속확률분포도 유용하게 활용 가능 

* `dnorm`

* `qnorm` : gives quantile

```{r}
x <- seq(-4, 4, length.out = 100)
qplot(x, f, geom = "line", data = data.frame(x, f = dnorm(x)))
```

* `pnorm`

* `rnorm` 

For the student-t, described later in Section 16.10, the shorthand t is used so the functions are dt for the density, qt for the quantiles, pt for the cumulative distribution function, and rt for Monte Carlo simulation.

***
# **13.14 Exercises

1. Assume the distribution of female heights is approximated by a normal distribution with a mean of 64 inches and a standard deviation of 3 inches. If we pick a female at random, what is the probability that she is 5 feet or shorter?

```{r}
# 5feet = 5*12inches
pnorm(60, 64, 3)
```

2. Assume the distribution of female heights is approximated by a normal distribution with a mean of 64 inches and a standard deviation of 3 inches. If we pick a female at random, what is the probability that she is 6 feet or taller?

```{r}
1 - pnorm(72, 64, 3)
```

3. Assume the distribution of female heights is approximated by a normal distribution with a mean of 64 inches and a standard deviation of 3 inches. If we pick a female at random, what is the probability that she is between 61 and 67 inches?

```{r}
pnorm(67, 64, 3) - pnorm(61, 64, 3)
```

4. Repeat the exercise above, but convert everything to centimeters. That is, multiply every height, including the standard deviation, by 2.54. What is the answer now?

```{r}
m = 64*2.54
s = 3*2.54
```

```{r}
pnorm(67*2.54, m, s) - pnorm(61*2.54, m, s)
```

* 동일하다. 

5. Notice that the answer to the question does not change when you change units. This makes sense since the answer to the question should not be affected by what units we use. In fact, if you look closely, you notice that 61 and 64 are both 1 SD away from the average. Compute the probability that a randomly picked, normally distributed random variable is within 1 SD from the average.

```{r}
m = sample(x=1:100, size=1)
s = sample(x=1:100, size=1)
pnorm(m+s, m, s) - pnorm(m-s, m, s)
```

6. To see the math that explains why the answers to questions 3, 4, and 5 are the same, suppose we have a random variable with average $m$ and standard error $s$. Suppose we ask the probability of $X$  being smaller or equal to  $a$. Remember that, by definition, $a$ is $(a-m)/s$ standard deviations $s$ away from the average $m$. The probability is :

$Pr(X<=a)$

Now we subtract $μ$ to both sides and then divide both sides by $σ$:

$Pr((X-m)/s <= (a-m)/s)$

The quantity on the left is a standard normal random variable. It has an average of 0 and a standard error of 1. We will call it $Z$ : 

$Pr(Z<=(a-m)/s)$

So, no matter the units, the probability of $X<=a$ is the same as the probability of a standard normal variable being less than $(a-m)/s$. If `mu` is the average and `sigma` the standard error, which of the following R code would give us the right answer in every situation:

a. mean(X<=a)
**b. pnorm((a - m)/s)**
c. pnorm((a - m)/s, m, s)
d. pnorm(a)

7.  Imagine the distribution of male adults is approximately normal with an expected value of 69 and a standard deviation of 3. How tall is the male in the 99th percentile? Hint: use qnorm.

```{r}
qnorm(0.99, 69, 3)
```

8. The distribution of IQ scores is approximately normally distributed. The average is 100 and the standard deviation is 15. Suppose you want to know the distribution of the highest IQ across all graduating classes if 10,000 people are born each in your school district. Run a Monte Carlo simulation with B=1000 generating 10,000 IQ scores and keeping the highest. Make a histogram.

```{r}
B <- 10000
highest_IQ <- replicate(B, {
  simulated_data <- rnorm(10000, 100, 15)
  max(simulated_data)
})

ggplot(NULL, aes(x=highest_IQ)) + geom_histogram() 
```

