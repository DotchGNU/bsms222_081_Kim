---
title: "Chap 15. Statistical inference"
output: html_notebook
---

* Statistical inference : the part of statistics that helps distinguish patterns arising from signal from those arising from chance 

```
In Chapter 16 we will describe, in some detail, how poll aggregators such as FiveThirtyEight use data to predict election outcomes. To understand how they do this, we first need to learn the basics of Statistical Inference, the part of statistics that helps distinguish patterns arising from signal from those arising from chance. Statistical inference is a broad topic and here we go over the very basics using polls as a motivating example. To describe the concepts, we complement the mathematical formulas with Monte Carlo simulations and R code.
```

***

# **15.1 Polls**

* Poll 여론조사 : 작은 집단을 대상으로 조사한 후 이를 바탕으로 전체 집단의 의견을 추정하는 것 

* "Inference" : 위의 과정을 진행하는 통계학적인 이론 

예) 대통령 선거에서  당선 예측 

* Terms in polls 

1) Spread : the estimated difference between support for the two candidates

2) MoE : Marging of error 

3) estimates 

4) Confidence intervals

5) p-values 

* final goal : to understand probabilistic statements about the probability of a candidate winning, we will have about **Bayesian modeling**

```
Opinion polling has been conducted since the 19th century. The general goal is to describe the opinions held by a specific population on a given set of topics. In recent times, these polls have been pervasive during presidential elections. Polls are useful when interviewing every member of a particular population is logistically impossible. The general strategy is to interview a smaller group, chosen at random, and then infer the opinions of the entire population from the opinions of the smaller group. Statistical theory is used to justify the process. This theory is referred to as inference and it is the main topic of this chapter.

Perhaps the best known opinion polls are those conducted to determine which candidate is preferred by voters in a given election. Political strategists make extensive use of polls to decide, among other things, how to invest resources. For example, they may want to know in which geographical locations to focus their “get out the vote” efforts.

Elections are a particularly interesting case of opinion polls because the actual opinion of the entire population is revealed on election day. Of course, it costs millions of dollars to run an actual election which makes polling a cost effective strategy for those that want to forecast the results.

Although typically the results of these polls are kept private, similar polls are conducted by news organizations because results tend to be of interest to the general public and made public. We will eventually be looking at such data.

Real Clear Politics50 is an example of a news aggregator that organizes and publishes poll results. For example, they present the following poll results reporting estimates of the popular vote for the 2016 presidential election51: 

[table]

Although in the United States the popular vote does not determine the result of the presidential election, we will use it as an illustrative and simple example of how well polls work. Forecasting the election is a more complex process since it involves combining results from 50 states and DC and we describe it in Section 16.8.

Let’s make some observations about the table above. First, note that different polls, all taken days before the election, report a different spread: the estimated difference between support for the two candidates. Notice also that the reported spreads hover around what ended up being the actual result: Clinton won the popular vote by 2.1%. We also see a column titled MoE which stands for margin of error.

In this section, we will show how the probability concepts we learned in the previous chapter can be applied to develop the statistical approaches that make polls an effective tool. We will learn the statistical concepts necessary to define estimates and margins of errors, and show how we can use these to forecast final results relatively well and also provide an estimate of the precision of our forecast. Once we learn this, we will be able to understand two concepts that are ubiquitous in data science: confidence intervals and p-values. Finally, to understand probabilistic statements about the probability of a candidate winning, we will have to learn about Bayesian modeling. In the final sections, we put it all together to recreate the simplified version of the FiveThirtyEight model and apply it to the 2016 election.

We start by connecting probability theory to the task of using polls to learn about a population.
```

## 15.1.1 The sampling model for polls 

* Make the example 

```{r}
library(tidyverse)
library(dslabs)
take_poll(25) #take random poll,function in dslabs
```

* Assume there are just two parties : Republican(Blue), Democratic(Red)

```
To help us understand the connection between polls and what we have learned, let’s construct a similar situation to the one pollsters face. To mimic the challenge real pollsters face in terms of competing with other pollsters for media attention, we will use an urn full of beads to represent voters and pretend we are competing for a $25 dollar prize. The challenge is to guess the spread between the proportion of blue and red beads in this urn (in this case, a pickle jar):

Before making a prediction, you can take a sample (with replacement) from the urn. To mimic the fact that running polls is expensive, it costs you $0.10 per each bead you sample. Therefore, if your sample size is 250, and you win, you will break even since you will pay $25 to collect your $25 prize. Your entry into the competition can be an interval. If the interval you submit contains the true proportion, you get half what you paid and pass to the second phase of the competition. In the second phase, the entry with the smallest interval is selected as the winner.

The dslabs package includes a function that shows a random draw from this urn:.

[image above]

Think about how you would construct your interval based on the data shown above.

We have just described a simple sampling model for opinion polls. The beads inside the urn represent the individuals that will vote on election day. Those that will vote for the Republican candidate are represented with red beads and the Democrats with the blue beads. For simplicity, assume there are no other colors. That is, that there are just two parties: Republican and Democratic.
```

***

# **15.2 Populations, samples, parameters, and estimates**

* prediction : the proportion of blue beads in the urn $= p = Pr(blue\ beads)$

* $1-p = Pr(red\ beads)$

* spread : $p-(1-p) = 2p-1$

* Population : the beads in the urn

* Parameter : the proportion of blue beads in the population $p$

* Sample : the 25 beads we see in the previous plot 

> Are we ready to predict with certainty that there are more red beads than blue in the jar?

 - we have only one $p$ using only the information we observe. 
 
 - Sample proportion is a **"random variable"**
  : `take_poll(25)`를 여러 번 돌리면 그때마다 다른 $p$값을 걷게될 것 

```
We want to predict the proportion of blue beads in the urn. Let’s call this quantity   p  , which then tells us the proportion of red beads   1 − p  , and the spread   p − ( 1 − p )  , which simplifies to   2 p − 1  . 
In statistical textbooks, the beads in the urn are called the population. The proportion of blue beads in the population   p   is called a parameter. The 25 beads we see in the previous plot are called a sample. The task of statistical inference is to predict the parameter   p   using the observed data in the sample. 
Can we do this with the 25 observations above? It is certainly informative. For example, given that we see 13 red and 12 blue beads, it is unlikely that   p   > .9 or   p   < .1. But are we ready to predict with certainty that there are more red beads than blue in the jar? 
We want to construct an estimate of   p   using only the information we observe. An estimate should be thought of as a summary of the observed data that we think is informative about the parameter of interest. It seems intuitive to think that the proportion of blue beads in the sample   0.48   must be at least related to the actual proportion   p  . But do we simply predict   p   to be 0.48? First, remember that the sample proportion is a random variable. If we run the command take_poll(25) four times, we get a different answer each time, since the sample proportion is a random variable.
```

## 15.2.1 The sample average
 
* Sample average(표본평균의 평균, $\bar{X}$) 
 
$$
\bar{X}=1/N\times\sum_{i=1}^{N}X_i
$$

```
Conducting an opinion poll is being modeled as taking a random sample from an urn. We are proposing the use of the proportion of blue beads in our sample as an estimate of the parameter   p  . Once we have this estimate, we can easily report an estimate for the spread   2 p − 1  , but for simplicity we will illustrate the concepts for estimating   p  . We will use our knowledge of probability to defend our use of the sample proportion and quantify how close we think it is from the population proportion   p  .  We start by defining the random variable   X   as:   X = 1   if we pick a blue bead at random and   X = 0   if it is red. This implies that the population is a list of 0s and 1s. If we sample   N   beads, then the average of the draws   X 1 , … , X N   is equivalent to the proportion of blue beads in our sample. This is because adding the   X  s is equivalent to counting the blue beads and dividing this count by the total   N   is equivalent to computing a proportion. We use the symbol   ¯ X   to represent this average. In general, in statistics textbooks a bar on top of a symbol means the average. The theory we just learned about the sum of draws becomes useful because the average is a sum of draws multiplied by the constant   1 / N  :  ¯ X = 1 / N × N ∑ i = 1   X i    For simplicity, let’s assume that the draws are independent: after we see each sampled bead, we return it to the urn. In this case, what do we know about the distribution of the sum of draws? First, we know that the expected value of the sum of draws is   N   times the average of the values in the urn. We know that the average of the 0s and 1s in the urn must be   p  , the proportion of blue beads.  Here we encounter an important difference with what we did in the Probability chapter: we don’t know what is in the urn. We know there are blue and red beads, but we don’t know how many of each. This is what we want to find out: we are trying to estimate   p  .
```
 
## 15.2.2 Parameters

* parameter : to define unknown parts of our models 

- Since our main goal is figuring out what is $p$, we are going to estimate this parameter. 

```
Just like we use variables to define unknowns in systems of equations, in statistical inference we define parameters to define unknown parts of our models. In the urn model which we are using to mimic an opinion poll, we do not know the proportion of blue beads in the urn. We define the parameters   p   to represent this quantity.   p   is the average of the urn because if we take the average of the 1s (blue) and 0s (red), we get the proportion of blue beads. Since our main goal is figuring out what is   p  , we are going to estimate this parameter.  The ideas presented here on how we estimate parameters, and provide insights into how good these estimates are, extrapolate to many data science tasks. For example, we may want to determine the difference in health improvement between patients receiving treatment and a control group. We may ask, what are the health effects of smoking on a population? What are the differences in racial groups of fatal shootings by police? What is the rate of change in life expectancy in the US during the last 10 years? All these questions can be framed as a task of estimating a parameter from a sample. 
```

## 15.2.3 Polling versus forecasting 

* people's opinions fluctuate through time -> $p$ might be change

* Forcasting : Poll을 바탕으로 시간이 지남에 따라 어떻게 $p$가 변화하는지 모델을 만들어 당일의 결과를 예측하는 것. 

```
Before we continue, let’s make an important clarification related to the practical problem of forecasting the election. If a poll is conducted four months before the election, it is estimating the   p   for that moment and not for election day. The   p   for election night might be different since people’s opinions fluctuate through time. The polls provided the night before the election tend to be the most accurate since opinions don’t change that much in a day. However, forecasters try to build tools that model how opinions vary across time and try to predict the election night results taking into consideration the fact that opinions fluctuate. We will describe some approaches for doing this in a later section. 
```

## 15.2.4 Properties of our estimate : expected value and standard error 

* sample proportion $\bar{X}$ (the sum of independent draws) 

- **$\bar{X}$ is the parameter of interest $p$**

* expected value 

$$
N\bar{X} = N\times mean(p) \\
E(\bar{X}) = p 
$$

* standard error of the average

- incresing $N$, than decrease the standard error (the law of large numbers)

- we can calculate what is the "large enough". (e.g.$SE(\bar{X})<0.01$)

$$
SE(\bar{X}) = \sqrt{p(1-p)/N}
$$
```{r}
p=0.51
sqrt(p*(1-p))/sqrt(1000)
```


***

# **15.3 Exercises**

1. Suppose you poll a population in which a proportion $p$ of voters are Democrats and $1-p$ are Republicans. Your sample size is $N=25$. Consider the random variable $S$ which is the **total** number of Democrats in your sample. What is the expected value of this random variable? 

$$
S= E(N \times\bar{X}) = N \times E(\bar{X}) = N \times p = 25p
$$

2. What is the standard error of $S$?

$$
SE(S) = SE(N\times\bar{X}) = N \times SE(\bar{X}) = N * \sqrt{p(1-p)/N}
$$

3. Consider the random variable $S/N$. This is equivalent to the sample average, which we have been denoting as $\bar{X}$. What is the expected value of the $\bar{X}$?

$p$


4. What is the standard error of $\bar{X}$?

$$
\sqrt{p(1-p)/N}
$$

5. Write a line of code that gives you the standard error `se` for the problem above for several values of $p$, specifically for `p <- seq(0, 1, length = 100).` Make a plot of `se` versus `p`.

```{r}
p <- seq(0, 1, length = 100)
se <- function(p, N){
  sqrt(p*(1-p)/N)
}
p_se <- se(p, 25)
qplot(p, p_se)
```

6. Copy the code above and put it inside a for-loop to make the plot for N=25, N=100, and N=1000

```{r}
for(N in c(25, 100, 1000)){
  p_se <- se(p, N)
  plot(p, p_se)
}
```

7. If we are interested in  the difference in proportions, $p-(1-p)$, our estimate is $d=\bar{X}-(1-\bar{X})$. Use the rules we learned about sums of random variables and scaled random variables to derive the expected value of $d$.

$$
E(d)=E(\bar{X}-(1-\bar{X}))=E(2\bar{X}-1)=2E(\bar{X})-1 = 2p-1
$$

8. What is the standard error of $d$?

$$
SE(d)=2SE(\bar{X})-1=2\sqrt{p(1-p)/N}
$$

9. If the actual $p=0.45$, it means the Republicans are winning by a relatively large margin since $d=-0.1$, which is a 10% margin of victory. In this case, what is the standard error of $2\hat{X}-1$ if we take a sample of $N=25$?

```{r}
N <- 25
p <- 0.45
2*sqrt(p*(1-p)/N)
```

10. Given the answer to 9, which of the following best describes your strategy of using best describes your strategy of using a sample size of $N=25$? 

a. The expected value of our estimate $2\bar{X}-1$ is $d$, so our prediction will be right on. 

**b. Our standard error is larger than difference, so the chances of $2\bar{X}-1$ being positive and throwing us off were not that small. We should pick a large sample size. **

c. The difference is 10% and the standard error is about 0.2, therefore much smaller than the difference. 

d. Because we don’t know $p$, we have no way of knowing that making  $N$ larger would actually improve our standard error.


***

# **15.4 Central Limit Theorem in practice 

When N is large enough, 

$$
\bar{X} \sim N(p, \sqrt{p\times(1-p)/N}) 
$$
Now how does this help us? Suppose we want to know what is the probability that we are within 1% from  $p$. We are basically asking what is : $Pr(|\bar{X}-p| <= 0.01)$

$$
Pr(|\bar{X}-p| \leqq 0.01)\\
\Leftrightarrow Pr(\bar{X} \leqq p+0.01) - Pr(\bar{X} \leqq p-0.01)\\
\Leftrightarrow Pr(Z \leqq 0.01/SE(\bar{X})) - Pr(X \leqq -0.01/SE(\bar{X}))
$$


One problem we have is that since we don’t know  $p$, we don’t know $SE(\bar{X})$. But it turns out that **the CLT still works** if we estimate the standard error by using  $\bar{X}$  in place of  $p$. We say that we plug-in the estimate. Our estimate of the standard error is therefore:

$$
\hat{SE}(\bar{X})=\sqrt{\bar{X}(1-\bar{X})/N}
$$

In our first sample we had 12 blue and 13 red so $\bar{X}=0.48$ and our estimate of standard error is:



```{r}
x_hat <- 0.48 #hat refer to estimates
se <- sqrt(x_hat*(1-x_hat)/25)
se
```

and now we can answer the question of the probability of being close to $p$. The answer is:

추정값---> 실제값

```{r}
pnorm(0.01/se) - pnorm(-0.01/se)
```

$$
\Rightarrow N=25\ is\ not\ large\ enough
$$

Earlier we mentioned **the margin of error**. Now we can define it because it is simply two times the standard error, which we can now estimate. In our case it is:

```{r}
1.96*se
```

Why do we multiply by 1.96? Because if you ask what is the probability that we are within 1.96 standard errors from  $p$, we get:

$$
Pr(Z\leqq1.96SE(\bar{X})/SE(\bar{X}))-Pr(Z\leqq-1.96SE(\bar{X})/SE(\bar{X}))\\
\Leftrightarrow Pr(Z\leqq1.96)-Pr(Z\leqq-1.96)
$$

which we know is about 95%

```{r}
pnorm(1.96)-pnorm(-1.96)
```

In summary, the CLT tells us that our poll based on a sample size of 25 is not very useful. We don’t really learn much when the margin of error is this large. All we can really say is that the popular vote will not be won by a large margin. This is why pollsters tend to use larger sample sizes.

## 15.4.1 A Monte Carlo simulation

Suppose we want to use a Monte Carlo simulation to corroborate the tools we have built using probability theory. To create the simulation, we would write code like this:

```{r}
B <- 10000
n <- 1000
x_hat <- replicate(B, {
  x <- sample(c(0,1), size = N, replace = TRUE, prob = c(1-p,p))
  mean(x)
})
```

The problem is, of course, we don’t know `p`. We could construct an urn like the one pictured above and run an analog (without a computer) simulation. It would take a long time, but you could take 10,000 samples, count the beads and keep track of the proportions of blue. We can use the function `take_poll(n=1000)` instead of drawing from an actual urn, but it would still take time to count the beads and enter the results.

One thing we therefore do to corroborate theoretical results is to pick one or several values of `p` and run the simulations. Let’s set `p=0.45`. We can then simulate a poll:

```{r}
p <- 0.45
N <- 1000

x <- sample(c(0,1), size = N, replace = TRUE, prob = c(1-p, p))
x_hat <- mean(x)
```

In this particular sample, our estimate is x_hat. We can use that code to do a Monte Carlo simulation

```{r}
B <- 10000
x_hat <- replicate(B, {
  x <- sample(c(0,1), size = N, replace = TRUE, prob = c(1-p, p))
  mean(x)
})
```

To review, the theory tells us that $\bar{X}$ is approximately normally distributed, has expected value  $p=0.45$ and standard error $\sqrt{p(1-p)/N}=0.016$. The simulation confirms this:

```{r}
mean(x_hat)
```

```{r}
sd(x_hat)
```

## 15.4.2 The spread

The competition is to predict the spread, not the proportion   p  . However, because we are assuming there are only two parties, we know that the spread is   p − ( 1 − p ) = 2 p − 1  . As a result, everything we have done can easily be adapted to an estimate of   2 p − 1  . Once we have our estimate   ¯ X   and   ^ SE ( ¯ X )  , we estimate the spread with   2 ¯ X − 1   and, since we are multiplying by 2, the standard error is   2 ^ SE ( ¯ X )  . Note that subtracting 1 does not add any variability so it does not affect the standard error.  For our 25 item sample above, our estimate   p   is .48 with margin of error .20 and our estimate of the spread is 0.04 with margin of error .40. Again, not a very useful sample size. However, the point is that once we have an estimate and standard error for   p  , we have it for the spread   2 p − 1  


## 15.4.3 Bias : why not run a very large poll? 

For realistic values of  $p$ , say from 0.35 to 0.65, if we run a very large poll with 100,000 people, theory tells us that we would predict the election perfectly since the largest possible margin of error is around 0.3%:

One reason is that running such a poll is very expensive. Another possibly more important reason is that theory has its limitations. Polling is much more complicated than picking beads from an urn. Some people might lie to pollsters and others might not have phones. But perhaps the most important way an actual poll differs from an urn model is that we actually don’t know for sure who is in our population and who is not. How do we know who is going to vote? Are we reaching all possible voters? Hence, even if our margin of error is very small, it might not be exactly right that our expected value is  
p
 . We call this bias. Historically, we observe that polls are indeed biased, although not by that much. The typical bias appears to be about 1-2%. This makes election forecasting a bit more interesting and we will talk about how to model this in a later chapter.

***

# **15.5 Exercises**

1. Write an urn model function that takes the proportion of Democrats $p$ and the sample size $N$ as arguments and returns the sample average if Democrats are 1s and Republicans are 0s. Call the fucntion `take_sample`.

```{r}
take_sample <- function(p, N){
  x <- sample(c(0,1), size=N, replace=TRUE, prob=c(1-p, p))
  mean(x)
}  #democrates are 1s and republicans are 0s
```

2. Now assume `p <- 0.45` and that your sample size is $N=100$. Take a sample 10,000 times and save the vector of `mean(X)-p` into an object called `errors`. 

```{r}
p <- 0.45
N <- 100
B <- 10000
errors <- replicate(B, take_sample(p, N)-p)
```

3. The vectors `errors` contains, for each simulated sample, the difference between the actual $p$ and our estimate $\bar{X}$. We refer to this difference as the error. Compute the average and make a histogram of the errors generated in the Monte Carlo simulation and select which of the following best describes their distributions :

```{r}
mean(errors)
```

```{r}
hist(errors)
```

a. The errors are all about 0.05.
b. The errors are all about -0.05.
**c. The errors are symmetrically distributed around 0.**
d. The errors range from -1 to 1.

4. The error $\bar{X}-p$ is a random variable. In practice, the error is not observed because we do not know $p$. Here we observe it because we constructed the simulation. What is the average size of the errror if we define the size by taking the absolute value $|\bar{X}-p|$? 

```{r}
mean(abs(errors))
```

5. The standard error is related to the typical size of the error we make when predicting. We say size because we just saw that the errors are centered around 0, so thus the average error value is 0. For mathematical reasons related to the Central Limit Theorem, we actually use the standard deviation of errors rather than the average of the absolute values to quantify the typical size. What is this standard deviation of the errors?

```{r}
sqrt(mean(errors^2))
```

6. The theory we just learned tells us what this standard deviation is going to be because it is the standard error of $\bar{X}$. What does theory tell us is the standard error of $\bar{X} for a sample size of 100?

```{r}
N <- 100
sqrt(p*(1-p)/N)
```

7. In practice, we don’t know  $p$ , so we construct an estimate of the theoretical prediction based by plugging in  $\bar{X}$  for  $p$ . Compute this estimate. Set the seed at 1 with `set.seed(1)`. 

```{r}
set.seed(1)
x <- sample(c(0,1), size=N, replace=TRUE, prob = c(1-p,p))
x_bar <- mean(x)
sqrt(x_bar*(1-x_bar)/N)
```

8. Note how close the standard error estimates obtained from the Monte Carlo simulation (exercise 5), the theoretical prediction (exercise 6), and the estimate of the theoretical prediction (exercise 7) are. The theory is working and it gives us a practical approach to knowing the typical error we will make if we predict $p$ with $\bar{X}$. Another advantage that the theoretical result provides is that it gives an idea of how large a sample size is required to obtain the precision we need. Earlier we learned that the largest standard errors occur for $p=0.5$. Create a plot of the largest standard error for $N$  ranging from 100 to 5,000. Based on this plot, how large does the sample size have to be to have a standard error of about 1%?

```{r}
N <- seq(100, 5000, 100)
p <- 0.5
se <- sqrt(p*(1-p)/N)
data_frame(N, se) %>% ggplot(aes(N, se)) + geom_point() + geom_hline(yintercept = 0.01)
```


a. 100
b. 500
**c. 2,500**
d. 4,000

9.  For sample size $N=100$, the central limit theorem tells us that the distribution of  $\bar{X}$  is:

a. practically equal to  $p$.

**b. approximately normal with expected value  $p$ and standard error $\sqrt{p(1−p)/N}$.**

c. approximately normal with expected value  $\bar{X}$  and standard error $\sqrt{\bar{X}(1−\bar{X})/N}$ .

d. not a random variable.


10. Based on the answer from exercise 8, the error  $\bar{X}−p$  is:

a. practically equal to 0.

b. approximately normal with expected value  0  and standard error $\sqrt{p(1−p)/N}$

**c. approximately normal with expected value  $p$  and standard error$\sqrt{p(1−p)/N}$**

d. not a random variable.

11. To corroborate your answer to exercise 9, make a qq-plot of the `errors` you generated in exercise 2 to see if they follow a normal distribution.

```{r}
qqnorm(errors)
```

12. If   $p = 0.45$   and   $N = 100$   as in exercise 2, use the CLT to estimate the probability that   $\bar{X} > 0.5$. You can assume you know   $p = 0.45$   for this calculation.

```{r}
p <- 0.45
N <- 100
1-pnorm(0.5, p, sqrt(p*(1-p)/N))
```

13. Assume you are in a practical situation and you don’t know  $p$ . Take a sample of size $N=100$  and obtain a sample average of $\bar{X}=0.51$. What is the CLT approximation for the probability that your error is equal to or larger than 0.01?

```{r}
x_bar <- 0.51
N <- 100
se_bar <- sqrt(x_bar*(1-x_bar)/N)
1 - pnorm(0.01, 0, se_bar) + pnorm(-0.01, 0, se_bar)
```

***

# **15.6 Confidence intervals

## 15.6.1 A Monte Carlo simulation

## 15.6.2 The correct language 

***

# **15.7 Exercises**

***

# **15.8 Power**

*** 

# **15.9 p-values**

***

# **15.10 Association tests**

## 15.10.1 Lady Testing Tea

## 15.10.2 Two-by-two tables

## 15.10.3 Chi-square Test

## 15.10.4 The odds ratio

## 15.10.5 Confidence intervals for the odds ratio

## 15.10.6 Small count correction

## 15.10.7 Large samples, small p-values

***

# **15.11 Exercises**




